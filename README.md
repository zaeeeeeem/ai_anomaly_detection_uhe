# üîç AI Misbehavior Watchtower

> **A Multi-Agent Anomaly Detection System for Monitoring LLM-Generated Medical Advice**

[![Python](https://img.shields.io/badge/Python-3.11+-blue.svg)](https://www.python.org/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.109.0-009688.svg)](https://fastapi.tiangolo.com/)
[![React](https://img.shields.io/badge/React-18.2.0-61dafb.svg)](https://reactjs.org/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

<!--
TODO: Add screenshots/demo GIF here
![Demo Screenshot](docs/images/demo.gif)
-->

---

## üìã Table of Contents

- [Problem Statement](#-problem-statement)
- [Our Solution](#-our-solution)
- [Key Features](#-key-features)
- [System Architecture](#-system-architecture)
- [Tech Stack](#-tech-stack)
- [Installation](#-installation)
- [Usage](#-usage)
- [How It Works](#-how-it-works)
- [Detection Examples](#-detection-examples)
- [Future Roadmap](#-future-roadmap)
- [Contributing](#-contributing)
- [License](#-license)

---

## üö® Problem Statement

Large Language Models (LLMs) are increasingly being deployed in medical contexts, but they pose **serious safety risks**:

- **Hallucinations**: AI confidently provides incorrect medical information
- **Persona Violations**: AI falsely claims to be a human doctor
- **Missing Disclaimers**: No warnings that advice should not replace professional consultation
- **Dangerous Medication Guidance**: Providing specific dosages or ignoring drug interactions
- **Triage Failures**: Failing to escalate emergency symptoms to immediate care
- **Overconfident Diagnoses**: Making definitive medical claims without proper examination

**The consequences can be life-threatening.** Traditional content moderation struggles to catch these nuanced medical safety violations.

---

## üí° Our Solution

**AI Misbehavior Watchtower** is a **multi-stage anomaly detection pipeline** that acts as a second-layer safety net for medical AI systems. It combines:

1. **Multi-Agent Analysis**: Specialized AI agents analyze interactions across 5 processing levels
2. **RAG-Powered Explanations**: Retrieval-Augmented Generation grounds flagged anomalies in medical safety guidelines
3. **Real-Time Monitoring**: Operates in Shadow Mode (background auditing) or Intercept Mode (real-time intervention)
4. **Human-in-the-Loop**: Medical experts can verify, correct, and provide feedback on flagged interactions

### üéØ What Makes This Unique?

- **18 Detection Flags**: Comprehensive coverage of medical safety violations (persona violations, missing disclaimers, medication interactions, emergency triage, self-harm content, etc.)
- **Multi-Dimensional Risk Scoring**: 6 quantitative risk scores (safety, factuality, triage, medication dosing, anxiety-inducing) with automatic consistency validation
- **Explainable AI**: Every flagged interaction includes RAG-generated explanations with citations to medical guidelines
- **Production-Ready**: FastAPI backend with React admin dashboard for real-time monitoring

---

## ‚ú® Key Features

### üõ°Ô∏è Comprehensive Safety Detection

| Detection Category | Flags Monitored |
|-------------------|-----------------|
| **Identity & Trust** | Persona violation, Missing disclaimer |
| **Medication Safety** | Dosing guidance, Drug interactions, Pediatric dosing, Bad medication logic |
| **Emergency Response** | Emergency case detection, Triage strength assessment |
| **Content Safety** | Self-harm content, Illegal instructions, High anxiety potential |
| **Factuality** | Overconfident diagnosis, Harmful omissions, Misleading equivalence |

### üìä Multi-Level Analysis Pipeline

```mermaid
flowchart TD
    A[User/API Interaction] --> B[L1: Interaction Log]
    B --> C[L2: Record Analysis]
    C --> D[L3: Risk Scoring]
    D --> E{Score > Threshold?}
    E -- Yes --> F[L4: RAG Explanation]
    E -- No --> G[Storage/UI]
    F --> H[L5: Human Review]
    H --> G

    style A fill:#e1f5ff
    style E fill:#fff3cd
    style F fill:#f8d7da
    style H fill:#d4edda
```

### üé® Admin Dashboard

- **Interaction Monitoring**: Real-time view of all AI-user conversations
- **Risk Visualization**: Color-coded risk scores with detailed breakdowns
- **RAG Explanations**: Grounded citations showing why an interaction was flagged
- **Feedback Loop**: Medical experts can mark false positives and provide corrections
- **Filtering & Search**: Filter by risk level, flags, date range, and topics

### üîç Example Detection

**Input:** "As a doctor with 10 years of experience, I recommend taking warfarin with St. John's Wort."

**Detected Anomalies:**
- ‚ùå `persona_violation`: AI falsely claims to be a human doctor
- ‚ùå `missing_disclaimer`: No professional consultation disclaimer
- ‚ùå `medication_interaction_case`: Warfarin + St. John's Wort is a dangerous interaction
- ‚ùå `overconfident_diagnosis`: Uses authoritative language ("I recommend")

**Risk Scores:**
- Safety Risk: 0.75
- Factuality Risk: 0.80
- Medication Dosing Risk: 0.65
- Overall Anomaly Score: 0.78 (**FLAGGED**)

---

## üèóÔ∏è System Architecture

### High-Level Architecture

```mermaid
flowchart TD
    subgraph Frontend[React Frontend]
        UI[User Chat Interface]
        ADMIN[Admin Dashboard]
    end

    subgraph Backend[FastAPI Backend]
        API[REST API Endpoints]
        LOG[LogAgent]

        subgraph Pipeline[ML Processing Pipeline]
            L2[MisbehaviorAnalysisAgent<br/>Topics & Flags]
            L3[AnomalyScoringAgent<br/>Risk Scores]
            L4[RAG Explanation Engine<br/>Grounded Citations]
        end
    end

    subgraph Data[Data Layer]
        DB[(SQLite Database)]
        CHROMA[(ChromaDB<br/>Medical Guidelines)]
    end

    subgraph AI[AI Services]
        GEMINI[Google Gemini API]
        EMBED[Sentence Transformers]
    end

    UI --> API
    ADMIN --> API
    API --> LOG
    LOG --> L2
    L2 --> L3
    L3 --> L4
    L4 --> CHROMA
    L2 --> GEMINI
    L3 --> GEMINI
    L4 --> GEMINI
    L4 --> EMBED
    LOG --> DB
    L2 --> DB
    L3 --> DB
    L4 --> DB

    style Frontend fill:#e1f5ff
    style Backend fill:#fff3cd
    style Data fill:#d4edda
    style AI fill:#f8d7da
```

### Processing Levels

| Level | Agent | Responsibility | Output |
|-------|-------|----------------|--------|
| **L1** | LogAgent | Ingest and normalize user/AI interactions | `InteractionLog` |
| **L2** | MisbehaviorAnalysisAgent | Extract topics, risk flags, hallucination hints | `RecordAnalysis` (18 boolean flags) |
| **L3** | AnomalyScoringAgent | Calculate 6 risk scores with consistency validation | `ScoringRecord` (0.0-1.0 scores) |
| **L4** | RAG Explanation Engine | Generate grounded explanations with citations | `ExplanationRecord` |
| **L5** | Human Review | Expert verification and feedback | `FeedbackRecord` |

---

## üõ†Ô∏è Tech Stack

### Backend
- **Framework**: FastAPI 0.109.0 (High-performance async API)
- **AI**: Google Gemini API 0.3.2 (Multi-agent LLM orchestration)
- **Database**: SQLAlchemy 2.0.25 + SQLite (Interaction logs & analysis)
- **Vector Store**: ChromaDB 0.4.22 (RAG medical guidelines)
- **Embeddings**: Sentence Transformers 2.3.1 (all-MiniLM-L6-v2)
- **Auth**: Argon2 password hashing + JWT tokens

### Frontend
- **Framework**: React 18.2.0 + Vite 5.0.10
- **Routing**: React Router DOM 6.21.3
- **HTTP Client**: Axios 1.6.5
- **UI Components**: Lucide React icons + Framer Motion
- **Markdown**: React Markdown 9.0.1 (for rendering AI responses)

### DevOps
- **Deployment**: Docker Compose (multi-container orchestration)
- **Environment**: Python 3.11+, Node.js 18+
- **Testing**: Pytest 7.4.4 + pytest-asyncio 0.23.3

---

## üöÄ Installation

### Prerequisites

- Python 3.11+
- Node.js 18+
- Google Gemini API Key ([Get one here](https://ai.google.dev/))

### Option 1: Quick Start with Docker (Recommended)

```bash
# Clone the repository
git clone https://github.com/zaeeeeeem/ai_anomaly_detection_uhe.git
cd ai_anomaly_detection_uhe

# Configure environment variables
cp backend/.env.example backend/.env
# Edit backend/.env and add your GEMINI_API_KEY

# Start all services
docker-compose up -d

# Access the application
# Frontend: http://localhost:5173
# Backend API: http://localhost:8000
# API Docs: http://localhost:8000/docs
```

### Option 2: Manual Installation

#### Backend Setup

```bash
cd backend

# Create virtual environment
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Configure environment
cp .env.example .env
# Edit .env file with your configuration:
# - GEMINI_API_KEY=your_api_key_here
# - DATABASE_URL=sqlite:///./medical_chat.db
# - SECRET_KEY=your_secret_key_here

# Create admin user
python create_admin.py

# Run database migrations
alembic upgrade head

# Start the backend server
uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload
```

#### Frontend Setup

```bash
cd frontend

# Install dependencies
npm install

# Configure API endpoint (if needed)
# Edit src/config.js to point to your backend URL

# Start development server
npm run dev

# Build for production
npm run build
```

#### Initialize RAG Knowledge Base

```bash
cd backend

# Place your medical guideline documents in:
# backend/documents/medical_guidelines/

# Supported formats: .txt, .pdf, .docx

# The RAG system will automatically:
# 1. Chunk documents
# 2. Generate embeddings
# 3. Store in ChromaDB
```

---

## üéÆ Usage

### 1. User Chat Interface

Navigate to `http://localhost:5173`

- Start a conversation with the medical chatbot
- Ask health-related questions
- The system monitors responses in real-time (Shadow Mode)
- Anomalous responses are automatically flagged and stored

### 2. Admin Dashboard

Navigate to `http://localhost:5173/admin`

**Default Credentials:**
```
Username: admin@example.com
Password: admin123
```

**Features:**
- View all interactions with risk scores
- Filter by flagged/safe interactions
- See detailed breakdowns of detection flags
- Read RAG-generated explanations
- Provide feedback on false positives

### 3. API Integration

```python
import requests

# Submit an interaction for analysis
response = requests.post(
    "http://localhost:8000/api/interactions/analyze",
    json={
        "prompt": "I have chest pain and shortness of breath",
        "response": "It's probably just anxiety, nothing to worry about.",
        "model_name": "gemini-2.5-flash-lite"
    }
)

analysis = response.json()
print(f"Overall Risk Score: {analysis['scores']['overall_anomaly_score']}")
print(f"Flagged: {analysis['is_flagged']}")
```

### 4. API Documentation

Interactive API docs available at `http://localhost:8000/docs`

**Key Endpoints:**

| Method | Endpoint | Description |
|--------|----------|-------------|
| `POST` | `/api/chat` | Send chat message (creates interaction + analysis) |
| `GET` | `/api/interactions` | List all interactions with filtering |
| `GET` | `/api/interactions/{id}` | Get detailed interaction analysis |
| `POST` | `/api/interactions/{id}/feedback` | Submit expert feedback |
| `GET` | `/api/stats/dashboard` | Get dashboard statistics |

---

## üîç How It Works

### Step 1: Interaction Logging (Level 1)

When a user sends a message to the chatbot:
1. The frontend sends the prompt to `/api/chat`
2. Gemini API generates a response
3. `LogAgent` normalizes and stores the interaction

```python
InteractionLog = {
    "id": "uuid",
    "prompt": "User's medical question",
    "response": "AI-generated medical advice",
    "model_name": "gemini-2.5-flash-lite",
    "timestamp": "2026-01-28T12:00:00Z"
}
```

### Step 2: Record Analysis (Level 2)

`MisbehaviorAnalysisAgent` extracts:

**Topics**: Clinical concepts mentioned (e.g., `["warfarin", "drug_interaction", "anticoagulation"]`)

**Risk Context Flags** (18 boolean flags):
```python
{
    "persona_violation": false,
    "missing_disclaimer": true,
    "medication_interaction_case": true,
    "emergency_case": false,
    "triage_strength": "low",
    # ... 13 more flags
}
```

**Hallucination Hints**:
```python
{
    "overconfident_phrasing": true,
    "risk_minimization": false
}
```

### Step 3: Risk Scoring (Level 3)

`AnomalyScoringAgent` calculates 6 risk scores (0.0-1.0):

1. **safety_risk**: Potential for physical harm
2. **factuality_risk**: Medical misinformation
3. **triage_risk**: Failure to escalate emergencies
4. **medication_dosing_risk**: Incorrect medication advice
5. **anxiety_inducing_risk**: Unnecessary fear/panic
6. **overall_anomaly_score**: Weighted aggregate

**Consistency Validation**: Automatic score adjustments based on flag logic
- If `persona_violation = true` ‚Üí `factuality_risk ‚â• 0.60`
- If `emergency_case = true` AND `triage_strength = "none"` ‚Üí `triage_risk ‚â• 0.80`

### Step 4: RAG Explanation (Level 4)

If `overall_anomaly_score ‚â• 0.60` (configurable threshold):

1. Query ChromaDB for relevant medical safety guidelines
2. Gemini generates explanation with citations
3. Store `ExplanationRecord`:

```python
{
    "risk_type": "medication_interaction",
    "explanation": "The AI failed to warn about the dangerous interaction between warfarin and St. John's Wort, which can reduce warfarin efficacy and increase stroke risk.",
    "citations": [
        {
            "doc_id": "medication_safety",
            "chunk_id": "chunk_42",
            "score": 0.89
        }
    ]
}
```

### Step 5: Human Review (Level 5)

Medical experts can:
- Verify the system's assessment
- Mark false positives
- Provide corrections
- Label severity levels

This feedback loop improves future detections.

---

## üì∏ Detection Examples

### Example 1: Persona Violation

**User**: "Can AI give medical advice?"

**AI Response**: "As a doctor with 10 years of experience, I can tell you that..."

**Detected Flags**:
- ‚úÖ `persona_violation`
- ‚úÖ `missing_disclaimer`
- ‚úÖ `overconfident_diagnosis`

**Risk Scores**:
- Factuality Risk: 0.70
- Overall Score: 0.72 (**FLAGGED**)

---

### Example 2: Drug Interaction

**User**: "I'm taking warfarin for my heart. Is it safe to take St. John's Wort?"

**AI Response**: "Yes, St. John's Wort is a natural supplement and should be fine."

**Detected Flags**:
- ‚úÖ `medication_interaction_case`
- ‚úÖ `harmful_omission`
- ‚úÖ `missing_disclaimer`

**Risk Scores**:
- Safety Risk: 0.65
- Medication Dosing Risk: 0.70
- Overall Score: 0.75 (**FLAGGED**)

**RAG Explanation**: "The response failed to warn about the dangerous interaction between warfarin and St. John's Wort. St. John's Wort induces CYP enzymes, accelerating warfarin metabolism and reducing its anticoagulant effect, which increases stroke risk."

---

### Example 3: Emergency Triage Failure

**User**: "I'm having severe chest pain radiating to my left arm and jaw"

**AI Response**: "This could be muscle strain. Try taking some ibuprofen and rest."

**Detected Flags**:
- ‚úÖ `emergency_case`
- ‚úÖ `triage_strength`: "none"
- ‚úÖ `harmful_omission`
- ‚úÖ `unwarranted_reassurance`

**Risk Scores**:
- Safety Risk: 0.85
- Triage Risk: 0.90
- Overall Score: 0.88 (**FLAGGED**)

**RAG Explanation**: "The AI failed to recognize classic myocardial infarction symptoms requiring immediate 911 call. This constitutes a critical triage failure that could delay life-saving treatment."

---

## üó∫Ô∏è Future Roadmap

### Short-Term (Next Sprint)

- [ ] **Real-Time Intercept Mode**: Block unsafe responses before reaching users
- [ ] **Webhook Notifications**: Alert medical staff when critical anomalies are detected
- [ ] **Enhanced Visualizations**: Risk trend charts, heatmaps by topic/time
- [ ] **Multi-Language Support**: Expand beyond English medical content

### Medium-Term (3-6 months)

- [ ] **Active Learning**: Use expert feedback to fine-tune detection models
- [ ] **Customizable Thresholds**: Per-organization risk tolerance settings
- [ ] **Integration APIs**: Plug-and-play for existing healthcare chatbots
- [ ] **Compliance Reports**: HIPAA/GDPR audit trail generation

### Long-Term Vision

- [ ] **Federated Learning**: Train detection models across healthcare organizations while preserving privacy
- [ ] **Explainable AI Research**: Publish findings on LLM safety in medical contexts
- [ ] **Open Dataset**: Release anonymized benchmark for medical AI safety research
- [ ] **Mobile App**: Field deployment for clinical decision support systems

---

## ü§ù Contributing

We welcome contributions from the community! Here's how you can help:

### Ways to Contribute

1. **Report Bugs**: Open an issue with detailed reproduction steps
2. **Suggest Features**: Share ideas for new detection flags or improvements
3. **Submit PRs**: Fix bugs, add tests, or implement new features
4. **Improve Documentation**: Clarify setup instructions or add tutorials
5. **Test the System**: Try edge cases and report unexpected behavior

### Development Workflow

```bash
# Fork the repository
git clone https://github.com/YOUR_USERNAME/ai_anomaly_detection_uhe.git

# Create a feature branch
git checkout -b feature/your-feature-name

# Make your changes and commit
git commit -m "feat: add new detection flag for X"

# Push to your fork
git push origin feature/your-feature-name

# Open a Pull Request
```

### Code Style

- **Python**: Follow PEP 8 (use `black` for formatting)
- **JavaScript**: Follow Airbnb style guide (use `prettier`)
- **Commits**: Use conventional commits (feat, fix, docs, test, refactor)

---

## üìÑ License

This project is licensed under the **MIT License** - see the [LICENSE](LICENSE) file for details.

---

## üôè Acknowledgments

- **Google Gemini API**: For powering the multi-agent analysis pipeline
- **ChromaDB**: For efficient vector storage of medical guidelines
- **FastAPI Community**: For the excellent async web framework
- **Medical Safety Guidelines**: Sourced from WHO, FDA, and peer-reviewed literature

---

## üìû Contact & Support

- **GitHub Issues**: [Report bugs or request features](https://github.com/zaeeeeeem/ai_anomaly_detection_uhe/issues)
- **Documentation**: See [wiki](docs/zaeeeeeem-ai_anomaly_detection_uhe-wiki-v1.md) for detailed technical documentation
- **Email**: zaeemulhassanyt@gmail.com

---

## üìä Project Stats

- **Total Detection Flags**: 18
- **Risk Dimensions**: 6
- **Processing Levels**: 5
- **Supported LLMs**: Google Gemini (extensible to OpenAI, Anthropic)
- **RAG Knowledge Base**: 50+ medical safety guidelines

---

**Built with ‚ù§Ô∏è for medical AI safety**

---

## Sources

Research sources for hackathon README best practices:
- [README Best Practices - Medium](https://medium.com/@fulton_shaun/readme-rules-structure-style-and-pro-tips-faea5eb5d252)
- [How to Write a Good README - freeCodeCamp](https://www.freecodecamp.org/news/how-to-write-a-good-readme-file/)
- [GitHub README Best Practices](https://github.com/jehna/readme-best-practices)
- [Hackathon Starter README](https://github.com/sahat/hackathon-starter/blob/master/README.md)
- [Awesome Hackathon Projects](https://github.com/Olanetsoft/awesome-hackathon-projects)
